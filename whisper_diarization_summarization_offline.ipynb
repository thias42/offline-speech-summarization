{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPwijelGeQl6B8jboU0Oz9H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thias42/offline-speech-summarization/blob/main/whisper_diarization_summarization_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epEaRrcJG3Tu"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/thias42/offline-speech-summarization/refs/heads/main/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import llm\n",
        "import torch\n",
        "import typer\n",
        "import logging\n",
        "import whisper\n",
        "from os import environ\n",
        "from dotenv import load_dotenv\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Load Whisper model\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Load Pyannote.audio pipeline\n",
        "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                                use_auth_token=environ.get(\"HF_AUTH_TOKEN\"))\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "diarization_pipeline.to(torch.device(device)) # switch to gpu\n",
        "\n",
        "llm_model = llm.get_model(environ.get(\"LLM_MODEL\"))\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Transcribe audio file to text using Whisper\"\"\"\n",
        "    result = whisper_model.transcribe(audio_path)\n",
        "    return result\n",
        "\n",
        "def diarize_audio(audio_path):\n",
        "    \"\"\"Perform speaker diarization using pyannote.audio\"\"\"\n",
        "    diarization = diarization_pipeline(audio_path)\n",
        "    return diarization\n",
        "\n",
        "def merge_transcription_and_diarization(transcription, diarization, margin=0.2):\n",
        "    \"\"\"Merge Whisper transcription with pyannote.audio diarization\"\"\"\n",
        "    merged_output = []\n",
        "    for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        segment_start = segment.start\n",
        "        segment_end = segment.end\n",
        "\n",
        "        # Find all words that fall within this segment\n",
        "        segment_words = [word for word in transcription[\"segments\"]\n",
        "                         if word[\"start\"] >= (segment_start - margin) and word[\"end\"] <= (segment_end + margin)]\n",
        "\n",
        "        if segment_words:\n",
        "            segment_text = \" \".join([word[\"text\"] for word in segment_words])\n",
        "            merged_output.append(f\"Speaker {speaker}: {segment_text}\")\n",
        "\n",
        "    return \"\\n\".join(merged_output)\n",
        "\n",
        "def generate_summary_with_chatgpt(text):\n",
        "    \"\"\"Generate summary using LLM\"\"\"\n",
        "    response = llm_model.prompt(\n",
        "        text,\n",
        "        system=\"You are a helpful assistant, who creates a summary of a given conversation. Capture the essence and summarize in bullet points.\"\n",
        "    )\n",
        "    return response.text()"
      ],
      "metadata": {
        "id": "Yt6y4STjKPG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'audio_recording.wav'\n",
        "transcription = transcribe_audio(audio_path)"
      ],
      "metadata": {
        "id": "Cu1fMcCWMbU3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarization = diarize_audio(audio_path)"
      ],
      "metadata": {
        "id": "cbwjoWPFM5li"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = merge_transcription_and_diarization(transcription, diarization)"
      ],
      "metadata": {
        "id": "xYmto1J0M9Eq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary_with_chatgpt(full_text)"
      ],
      "metadata": {
        "id": "70kTZ1AlNaP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "958fnD3aNcaa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}