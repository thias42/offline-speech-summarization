{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP2u0czZdHopeazznW72tSD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thias42/offline-speech-summarization/blob/main/whisper_diarization_summarization_offline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "epEaRrcJG3Tu"
      },
      "outputs": [],
      "source": [
        "!wget https://raw.githubusercontent.com/thias42/offline-speech-summarization/refs/heads/main/requirements.txt\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import llm\n",
        "import torch\n",
        "import typer\n",
        "import logging\n",
        "import whisper\n",
        "from os import environ\n",
        "from dotenv import load_dotenv\n",
        "from pyannote.audio import Pipeline\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "# Load Whisper model\n",
        "whisper_model = whisper.load_model(\"base\")\n",
        "\n",
        "# Load Pyannote.audio pipeline\n",
        "diarization_pipeline = Pipeline.from_pretrained(\"pyannote/speaker-diarization@2.1\",\n",
        "                                                use_auth_token=environ.get(\"HF_AUTH_TOKEN\"))\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu'\n",
        "diarization_pipeline.to(torch.device(device)) # switch to gpu if available\n",
        "\n",
        "llm_model = llm.get_model(environ.get(\"LLM_MODEL\"))\n",
        "\n",
        "def transcribe_audio(audio_path):\n",
        "    \"\"\"Transcribe audio file to text using Whisper\"\"\"\n",
        "    result = whisper_model.transcribe(audio_path)\n",
        "    return result\n",
        "\n",
        "def diarize_audio(audio_path):\n",
        "    \"\"\"Perform speaker diarization using pyannote.audio\"\"\"\n",
        "    diarization = diarization_pipeline(audio_path)\n",
        "    return diarization\n",
        "\n",
        "def merge_transcription_and_diarization(transcription, diarization, margin=0.2):\n",
        "    \"\"\"Merge Whisper transcription with pyannote.audio diarization\"\"\"\n",
        "    merged_output = []\n",
        "    for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
        "        segment_start = segment.start\n",
        "        segment_end = segment.end\n",
        "\n",
        "        # Find all words that fall within this segment\n",
        "        segment_words = [word for word in transcription[\"segments\"]\n",
        "                         if word[\"start\"] >= (segment_start - margin) and word[\"end\"] <= (segment_end + margin)]\n",
        "\n",
        "        if segment_words:\n",
        "            segment_text = \" \".join([word[\"text\"] for word in segment_words])\n",
        "            merged_output.append(f\"Speaker {speaker}: {segment_text}\")\n",
        "\n",
        "    return \"\\n\".join(merged_output)\n",
        "\n",
        "def generate_summary(text):\n",
        "    \"\"\"Generate summary using LLM\"\"\"\n",
        "    response = llm_model.prompt(\n",
        "        text,\n",
        "        system=\"You are a helpful assistant, who creates a summary of a given conversation. Capture the essence and summarize in bullet points.\"\n",
        "    )\n",
        "    return response.text()"
      ],
      "metadata": {
        "id": "Yt6y4STjKPG1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "audio_path = 'audio_recording.wav'\n",
        "transcription = transcribe_audio(audio_path)"
      ],
      "metadata": {
        "id": "Cu1fMcCWMbU3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "diarization = diarize_audio(audio_path)"
      ],
      "metadata": {
        "id": "cbwjoWPFM5li"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_text = merge_transcription_and_diarization(transcription, diarization)"
      ],
      "metadata": {
        "id": "xYmto1J0M9Eq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary = generate_summary(full_text)"
      ],
      "metadata": {
        "id": "70kTZ1AlNaP-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Transcription:\\n{full_text}\\n')\n",
        "print(f'Summary:\\n{summary}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "958fnD3aNcaa",
        "outputId": "3c8df372-469a-4ef3-a9fa-401cfe69147a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Transcription:\n",
            "Speaker SPEAKER_00:  Alright music fans, let's dive into something that I think we can all relate to.  You know that feeling you get when you're trying to find the perfect track for the moment,  but you're just scrolling endlessly through these massive music libraries.  The algorithms are trying, but they just don't quite get it.\n",
            "Speaker SPEAKER_01:  Yeah, it's almost like trying to describe a color you've never seen before, right?\n",
            "Speaker SPEAKER_00:  Exactly.  You're trying to put this feeling, this vibe, this very specific musical need into words.  And then you end up somewhere in the land of genre filters, which...\n",
            "\n",
            "Summary:\n",
            "* The speakers discuss their frustration with finding the perfect track for a moment due to massive music libraries and algorithms that don't quite get it.\n",
            "* They compare this feeling to describing an unseen color or putting into words a very specific musical need.\n",
            "* They mention genre filters as a solution but find them ineffective, leading to frustration with finding the right track for their needs.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4Ia95QlsR2E2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}